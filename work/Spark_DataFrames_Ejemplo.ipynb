{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apache Spark: SQL/DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un DataFrame es una **representación relacional de los datos**. Los DataFrames son similares a las tablas relacionales o DataFrames en Python aunque con muchas optimizaciones que se ejecutan de manera \"oculta\" para el usuario."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importar pyspark (para el ejemplo 1, 2 y 3)\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utilidades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilidades\n",
    "def display(dfs: pyspark.sql.dataframe.DataFrame) -> None:\n",
    "    \"\"\"\n",
    "        Desplegar el schema y los datos \n",
    "        que contiene el dataframe pyspark\n",
    "    \"\"\"\n",
    "    dfs.printSchema()\n",
    "    dfs.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo 1\n",
    "Con datos estructurados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importar modulos de pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear la sesión de Spark \n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datos\n",
    "emp = [\n",
    "    (1, \"aa\", \"level1\", 2000),\n",
    "    (2, \"bb\", \"level1\", 2200),\n",
    "    (3, \"cc\", \"level2\", 5000),\n",
    "    (4, \"dd\", \"level2\", 6300),\n",
    "    (5, \"ff\", \"level3\", 7200),\n",
    "    (6, \"gg\", \"level3\", 9500)\n",
    "]\n",
    "\n",
    "# Crear el DataFrame\n",
    "dfs = spark.createDataFrame(emp, [\"id\", \"nombre\", \"dept\", \"salary\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+------+\n",
      "| id|nombre|  dept|salary|\n",
      "+---+------+------+------+\n",
      "|  1|    aa|level1|  2000|\n",
      "|  2|    bb|level1|  2200|\n",
      "|  3|    cc|level2|  5000|\n",
      "|  4|    dd|level2|  6300|\n",
      "|  5|    ff|level3|  7200|\n",
      "|  6|    gg|level3|  9500|\n",
      "+---+------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# display\n",
    "dfs.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se detiene la sesión\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo 2\n",
    "Con datos anidados (no estructurados)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importar modulos de pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from pyspark.sql.functions import (\n",
    "    explode as Fexplode,\n",
    "    explode_outer as Fexplode_outer,\n",
    "    posexplode as Fposexplode,\n",
    "    posexplode_outer as Fposexplode_outer,\n",
    "    flatten as Fflatten,\n",
    "    col as Fcol\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear la sesión de Spark\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "        .master(\"local[1]\")\n",
    "        .appName(\"PruebaSpark.com\")\n",
    "        .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "datos = [\n",
    "    ('James',['Java','Scala'],{'hair':'black','eye':'brown'}),\n",
    "    ('Michael',['Spark','Java',None],{'hair':'brown','eye':None}),\n",
    "    ('Robert',['CSharp',''],{'hair':'red','eye':''}),\n",
    "    ('Washington',None,None),\n",
    "    ('Jefferson',['1','2'],{})\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear el DataFrame\n",
    "dfs = spark.createDataFrame(\n",
    "    data=datos,\n",
    "    schema=['nombre','conocimiento_lenguajes','descripcion_fisica']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- nombre: string (nullable = true)\n",
      " |-- conocimiento_lenguajes: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- descripcion_fisica: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: string (valueContainsNull = true)\n",
      "\n",
      "+----------+----------------------+--------------------+\n",
      "|    nombre|conocimiento_lenguajes|  descripcion_fisica|\n",
      "+----------+----------------------+--------------------+\n",
      "|     James|         [Java, Scala]|{eye -> brown, ha...|\n",
      "|   Michael|   [Spark, Java, null]|{eye -> null, hai...|\n",
      "|    Robert|            [CSharp, ]|{eye -> , hair ->...|\n",
      "|Washington|                  null|                null|\n",
      "| Jefferson|                [1, 2]|                  {}|\n",
      "+----------+----------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# display\n",
    "display(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- nombre: string (nullable = true)\n",
      " |-- col: string (nullable = true)\n",
      "\n",
      "+---------+------+\n",
      "|   nombre|   col|\n",
      "+---------+------+\n",
      "|    James|  Java|\n",
      "|    James| Scala|\n",
      "|  Michael| Spark|\n",
      "|  Michael|  Java|\n",
      "|  Michael|  null|\n",
      "|   Robert|CSharp|\n",
      "|   Robert|      |\n",
      "|Jefferson|     1|\n",
      "|Jefferson|     2|\n",
      "+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Funcion explode Caso 1 con arreglo\n",
    "dfs2 = dfs.select(dfs.nombre, Fexplode(dfs.conocimiento_lenguajes))\n",
    "display(dfs2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- nombre: string (nullable = true)\n",
      " |-- key: string (nullable = false)\n",
      " |-- value: string (nullable = true)\n",
      "\n",
      "+-------+----+-----+\n",
      "| nombre| key|value|\n",
      "+-------+----+-----+\n",
      "|  James| eye|brown|\n",
      "|  James|hair|black|\n",
      "|Michael| eye| null|\n",
      "|Michael|hair|brown|\n",
      "| Robert| eye|     |\n",
      "| Robert|hair|  red|\n",
      "+-------+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Funcion explode Caso 2 con clave valor\n",
    "dfs3 = dfs.select(dfs.nombre, Fexplode(dfs.descripcion_fisica))\n",
    "display(dfs3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+\n",
      "|    nombre|   col|\n",
      "+----------+------+\n",
      "|     James|  Java|\n",
      "|     James| Scala|\n",
      "|   Michael| Spark|\n",
      "|   Michael|  Java|\n",
      "|   Michael|  null|\n",
      "|    Robert|CSharp|\n",
      "|    Robert|      |\n",
      "|Washington|  null|\n",
      "| Jefferson|     1|\n",
      "| Jefferson|     2|\n",
      "+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Funcion explode_outer Caso 1 con arreglo\n",
    "(\n",
    "    dfs.select(\n",
    "        dfs.nombre,\n",
    "        Fexplode_outer(dfs.conocimiento_lenguajes)\n",
    "    )\n",
    "    .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+-----+\n",
      "|    nombre| key|value|\n",
      "+----------+----+-----+\n",
      "|     James| eye|brown|\n",
      "|     James|hair|black|\n",
      "|   Michael| eye| null|\n",
      "|   Michael|hair|brown|\n",
      "|    Robert| eye|     |\n",
      "|    Robert|hair|  red|\n",
      "|Washington|null| null|\n",
      "| Jefferson|null| null|\n",
      "+----------+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Funcion explode_outer Caso 2 con clave valor\n",
    "(\n",
    "    dfs.select(\n",
    "        dfs.nombre,\n",
    "        Fexplode_outer(dfs.descripcion_fisica)\n",
    "    )\n",
    "    .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+------+\n",
      "|   nombre|pos|   col|\n",
      "+---------+---+------+\n",
      "|    James|  0|  Java|\n",
      "|    James|  1| Scala|\n",
      "|  Michael|  0| Spark|\n",
      "|  Michael|  1|  Java|\n",
      "|  Michael|  2|  null|\n",
      "|   Robert|  0|CSharp|\n",
      "|   Robert|  1|      |\n",
      "|Jefferson|  0|     1|\n",
      "|Jefferson|  1|     2|\n",
      "+---------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Funcion posexplode Caso 1 con arreglo\n",
    "(\n",
    "    dfs.select(\n",
    "        dfs.nombre,\n",
    "        Fposexplode(dfs.conocimiento_lenguajes)\n",
    "    )\n",
    "    .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+----+-----+\n",
      "| nombre|pos| key|value|\n",
      "+-------+---+----+-----+\n",
      "|  James|  0| eye|brown|\n",
      "|  James|  1|hair|black|\n",
      "|Michael|  0| eye| null|\n",
      "|Michael|  1|hair|brown|\n",
      "| Robert|  0| eye|     |\n",
      "| Robert|  1|hair|  red|\n",
      "+-------+---+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Funcion posexplode Caso 2 con clave valor\n",
    "(\n",
    "    dfs.select(\n",
    "        dfs.nombre,\n",
    "        Fposexplode(dfs.descripcion_fisica)\n",
    "    )\n",
    "    .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+------+\n",
      "|    nombre| pos|   col|\n",
      "+----------+----+------+\n",
      "|     James|   0|  Java|\n",
      "|     James|   1| Scala|\n",
      "|   Michael|   0| Spark|\n",
      "|   Michael|   1|  Java|\n",
      "|   Michael|   2|  null|\n",
      "|    Robert|   0|CSharp|\n",
      "|    Robert|   1|      |\n",
      "|Washington|null|  null|\n",
      "| Jefferson|   0|     1|\n",
      "| Jefferson|   1|     2|\n",
      "+----------+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Funcion posexplode_outer Caso 1 con arreglo\n",
    "(\n",
    "    dfs.select(\n",
    "        dfs.nombre,\n",
    "        Fposexplode_outer(dfs.conocimiento_lenguajes)\n",
    "    )\n",
    "    .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+----+-----+\n",
      "|    nombre| pos| key|value|\n",
      "+----------+----+----+-----+\n",
      "|     James|   0| eye|brown|\n",
      "|     James|   1|hair|black|\n",
      "|   Michael|   0| eye| null|\n",
      "|   Michael|   1|hair|brown|\n",
      "|    Robert|   0| eye|     |\n",
      "|    Robert|   1|hair|  red|\n",
      "|Washington|null|null| null|\n",
      "| Jefferson|null|null| null|\n",
      "+----------+----+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Funcion posexplode_outer Caso 2 con clave valor\n",
    "(\n",
    "    dfs.select(\n",
    "        dfs.nombre,\n",
    "        Fposexplode_outer(dfs.descripcion_fisica)\n",
    "    )\n",
    "    .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se detiene la sesión\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo 3\n",
    "Desde un archivo Json (con multiples lineas) y consultarlo como apache sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importar modulos de pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from pyspark.sql.types import (\n",
    "    StructType,\n",
    "    StructField,\n",
    "    IntegerType,\n",
    "    DoubleType,\n",
    "    StringType\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear la sesión de Spark \n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "        .master(\"local[1]\")\n",
    "        .appName(\"PruebaSparkJson.com\")\n",
    "        .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creacion de Schema\n",
    "json_array_schema = StructType([\n",
    "    StructField(\"zip_code\", IntegerType(), False),\n",
    "    StructField(\"latitude\", DoubleType(), False),\n",
    "    StructField(\"longitude\", DoubleType(), False),\n",
    "    StructField(\"city\", StringType(), False),\n",
    "    StructField(\"state\", StringType(), False),\n",
    "    StructField(\"county\", StringType(), False)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creacion del \n",
    "dfs = (\n",
    "    spark.read\n",
    "        .option(\"multiline\", \"true\")\n",
    "        .schema(json_array_schema)\n",
    "        .json(\"data/zipcodesUSDummy.json\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- zip_code: integer (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- county: string (nullable = true)\n",
      "\n",
      "+--------+---------+----------+----------+-----+---------+\n",
      "|zip_code| latitude| longitude|      city|state|   county|\n",
      "+--------+---------+----------+----------+-----+---------+\n",
      "|     501|40.922326|-72.637078|Holtsville|   NY|  Suffolk|\n",
      "|     544|40.922326|-72.637078|Holtsville|   NY|  Suffolk|\n",
      "|     601|18.165273|-66.722583|  Adjuntas|   PR| Adjuntas|\n",
      "|     602|18.393103|-67.180953|    Aguada|   PR|   Aguada|\n",
      "|     603|18.455913| -67.14578| Aguadilla|   PR|Aguadilla|\n",
      "|     604| 18.49352|-67.135883| Aguadilla|   PR|Aguadilla|\n",
      "|     605|18.465162|-67.141486| Aguadilla|   PR|Aguadilla|\n",
      "|     606|18.172947|-66.944111|   Maricao|   PR|  Maricao|\n",
      "|     610|18.288685|-67.139696|    Anasco|   PR|   Anasco|\n",
      "+--------+---------+----------+----------+-----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "display(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|State|count|\n",
      "+-----+-----+\n",
      "|   NY|    2|\n",
      "|   PR|    7|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Contar estados (state)\n",
    "col_target = \"State\"\n",
    "dfs.groupBy(col_target).count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Registrando una tabla para consultarla con apache sql\n",
    "tb_codigo_zip = \"tb_codigo_zip\"\n",
    "dfs.createOrReplaceTempView(\"tb_codigo_zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# script sql\n",
    "query = f\"\"\"\n",
    "SELECT \n",
    "    t.{col_target},\n",
    "    COUNT(0) AS count\n",
    "FROM {tb_codigo_zip} AS t\n",
    "GROUP BY t.State\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SELECT \n",
      "    t.State,\n",
      "    COUNT(0) AS count\n",
      "FROM tb_codigo_zip AS t\n",
      "GROUP BY t.State\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print query\n",
    "print(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lanzar consulta\n",
    "qdfs = spark.sql(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- State: string (nullable = true)\n",
      " |-- count: long (nullable = false)\n",
      "\n",
      "+-----+-----+\n",
      "|State|count|\n",
      "+-----+-----+\n",
      "|   NY|    2|\n",
      "|   PR|    7|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "display(qdfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
